#!/usr/bin/env python3
"""
Complete MLOps Pipeline for Travel Advisor Chatbot
Single script that handles training, deployment, monitoring, and chatbot launch
"""

import sys
import os
import time
import subprocess
import threading
import webbrowser
from datetime import datetime
from typing import Dict, List, Any

# Add project paths
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'src'))
sys.path.append(os.path.join(project_root, 'mlops'))

def print_banner():
    """Print the MLOps pipeline banner."""
    print("=" * 80)
    print("ğŸš€ TRAVEL ADVISOR CHATBOT - COMPLETE MLOps PIPELINE")
    print("=" * 80)
    print("ğŸ¯ Single Script for Training, Deployment, Monitoring & Chatbot")
    print("=" * 80)

def open_browser_links():
    """Open browser links for MLflow and monitoring dashboard."""
    print("\nğŸŒ Opening browser links...")
    
    # Wait a moment for services to fully start
    time.sleep(3)
    
    try:
        # Open MLflow UI
        print("ğŸ“Š Opening MLflow UI...")
        webbrowser.open("http://127.0.0.1:5000")
        print("âœ… MLflow UI opened in browser")
    except Exception as e:
        print(f"âš ï¸  Could not open MLflow UI: {e}")
    
    try:
        # Open monitoring dashboard
        print("ğŸ“ˆ Opening monitoring dashboard...")
        webbrowser.open("http://localhost:8502")
        print("âœ… Monitoring dashboard opened in browser")
    except Exception as e:
        print(f"âš ï¸  Could not open monitoring dashboard: {e}")
    
    try:
        # Open chatbot application
        print("ğŸ¤– Opening chatbot application...")
        webbrowser.open("http://localhost:8501")
        print("âœ… Chatbot application opened in browser")
    except Exception as e:
        print(f"âš ï¸  Could not open chatbot application: {e}")

def check_dependencies():
    """Check if all required dependencies are installed."""
    print("\nğŸ” Checking Dependencies...")
    
    # Map package names to import names
    required_packages = {
        'mlflow': 'mlflow',
        'streamlit': 'streamlit', 
        'pandas': 'pandas',
        'numpy': 'numpy',
        'scikit-learn': 'sklearn',
        'sentence-transformers': 'sentence_transformers',
        'transformers': 'transformers',
        'torch': 'torch',
        'nltk': 'nltk',
        'requests': 'requests'
    }
    
    missing_packages = []
    for package_name, import_name in required_packages.items():
        try:
            __import__(import_name)
            print(f"âœ… {package_name}")
        except ImportError:
            print(f"âŒ {package_name} - Missing")
            missing_packages.append(package_name)
    
    if missing_packages:
        print(f"\nâš ï¸  Missing packages: {', '.join(missing_packages)}")
        print("Please install them with: pip install " + " ".join(missing_packages))
        return False
    
    print("âœ… All dependencies are installed!")
    return True

def check_mlflow_server():
    """Check if MLflow server is running."""
    print("\nğŸ“Š Checking MLflow Server...")
    
    try:
        import requests
        response = requests.get("http://127.0.0.1:5000", timeout=2)
        if response.status_code == 200:
            print("âœ… MLflow server is running on http://127.0.0.1:5000")
            return True
        else:
            print("âŒ MLflow server not responding")
            return False
    except:
        print("âŒ MLflow server not running")
        return False

def start_mlflow_server():
    """Start MLflow server in a new Windows terminal window."""
    print("\nğŸš€ Starting MLflow Server...")
    
    try:
        # Create necessary directories
        os.makedirs("mlruns", exist_ok=True)
        
        print("ğŸ”„ Starting MLflow server in new Windows terminal...")
        
        # MLflow command (using working path format)
        mlflow_cmd = "mlflow ui --backend-store-uri file:///D:/Travel%20Agent%20Chat%20Bot/mlruns --default-artifact-root file:///D:/Travel%20Agent%20Chat%20Bot/mlruns --host 127.0.0.1 --port 5000"
        
        # Start MLflow server in new Windows terminal window
        if os.name == 'nt':  # Windows
            cmd = f'start "MLflow Server" cmd /k "{mlflow_cmd}"'
            subprocess.Popen(cmd, shell=True)
            print("ğŸ“ Opened MLflow server in new Windows terminal window")
        else:  # Unix-like systems
            cmd = f'xterm -e "{mlflow_cmd}" &'
            subprocess.Popen(cmd, shell=True)
            print("ğŸ“ Opened MLflow server in new terminal window")
        
        # Wait for server to start
        print("â³ Waiting for MLflow server to start...")
        for i in range(15):  # Wait up to 15 seconds
            time.sleep(1)
            if check_mlflow_server():
                print("âœ… MLflow server started successfully!")
                print("ğŸ“Š MLflow UI available at: http://127.0.0.1:5000")
                print("ğŸ–¥ï¸  MLflow server is running in a new terminal window")
                return True
            print(f"   Attempt {i+1}/15...")
        
        print("âŒ Failed to start MLflow server within timeout")
        print("ğŸ’¡ Please check if a new terminal window opened for MLflow")
        print("ğŸ’¡ Or start MLflow server manually:")
        print(f"   {mlflow_cmd}")
        return False
            
    except Exception as e:
        print(f"âŒ Error starting MLflow server: {e}")
        print("ğŸ’¡ Please start MLflow server manually:")
        print("   mlflow ui --backend-store-uri file:///D:/Travel%20Agent%20Chat%20Bot/mlruns --default-artifact-root file:///D:/Travel%20Agent%20Chat%20Bot/mlruns --host 127.0.0.1 --port 5000")
        return False

def train_models():
    """Train all chatbot models with MLOps tracking."""
    print("\nğŸ“ Training Chatbot Models...")
    print("-" * 50)
    
    try:
        # Import and run training
        from mlops.scripts.train_chatbot_models import ChatbotModelTrainer
        
        trainer = ChatbotModelTrainer()
        
        # Load data and initialize models
        if not trainer.load_training_data():
            print("âŒ Failed to load training data")
            return False
        
        trainer.initialize_models()
        
        # Train all models
        success = trainer.train_all_models()
        
        if success:
            print("\nâœ… All models trained successfully!")
            trainer.evaluate_models()
            trainer.save_training_results()
            return True
        else:
            print("âŒ Model training failed")
            return False
            
    except Exception as e:
        print(f"âŒ Error during training: {e}")
        return False

def deploy_models():
    """Deploy trained models locally (no Docker)."""
    print("\nğŸš€ Deploying Models Locally...")
    print("-" * 35)
    
    try:
        # Create models directory if it doesn't exist
        os.makedirs("models", exist_ok=True)
        
        # Check if models were trained
        models_dir = "models"
        if os.path.exists(models_dir):
            model_files = [f for f in os.listdir(models_dir) if f.endswith(('.pkl', '.joblib', '.pt', '.pth'))]
            if model_files:
                print(f"âœ… Found {len(model_files)} trained models")
                print("âœ… Models are ready for local deployment")
                return True
            else:
                print("âš ï¸  No trained models found, but models directory exists")
                print("âœ… Local deployment setup complete")
                return True
        else:
            print("âœ… Created models directory for local deployment")
            return True
            
    except Exception as e:
        print(f"âŒ Error during local deployment: {e}")
        return False

def start_monitoring():
    """Start working monitoring dashboard."""
    print("\nğŸ“ˆ Starting Working Monitoring Dashboard...")
    
    try:
        print("ğŸ”„ Starting working monitoring dashboard...")
        
        # Use the new working monitoring dashboard
        monitoring_cmd = "streamlit run working_monitoring_dashboard.py --server.port 8502"
        
        # Start monitoring dashboard in new Windows terminal window
        if os.name == 'nt':  # Windows
            cmd = f'start "Working Monitoring Dashboard" cmd /k "{monitoring_cmd}"'
            subprocess.Popen(cmd, shell=True)
            print("ğŸ“ Opened working monitoring dashboard in new Windows terminal window")
        else:  # Unix-like systems
            cmd = f'xterm -e "{monitoring_cmd}" &'
            subprocess.Popen(cmd, shell=True)
            print("ğŸ“ Opened working monitoring dashboard in new terminal window")
        
        # Wait for dashboard to start
        print("â³ Waiting for monitoring dashboard to start...")
        for i in range(10):  # Wait up to 10 seconds
            time.sleep(1)
            try:
                import requests
                response = requests.get("http://localhost:8502", timeout=2)
                if response.status_code == 200:
                    print("âœ… Working monitoring dashboard started successfully!")
                    print("ğŸ“Š Monitoring dashboard available at: http://localhost:8502")
                    print("ğŸ–¥ï¸  Monitoring dashboard is running in a new terminal window")
                    print("ğŸ’¡ This dashboard shows real model data and simulated performance metrics")
                    return True
            except:
                pass
            print(f"   Attempt {i+1}/10...")
        
        print("âš ï¸  Monitoring dashboard may still be starting...")
        print("ğŸ“Š Monitoring dashboard should be available at: http://localhost:8502")
        return True
        
    except Exception as e:
        print(f"âš ï¸  Monitoring dashboard not available: {e}")
        print("ğŸ’¡ You can still use MLflow UI for monitoring: http://127.0.0.1:5000")
        return True

def launch_chatbot():
    """Launch the Streamlit chatbot application."""
    print("\nğŸ¤– Launching Travel Advisor Chatbot...")
    print("-" * 40)
    
    try:
        # Start chatbot app
        chatbot_cmd = ["streamlit", "run", "travel_chatbot_app.py", "--server.port", "8501"]
        
        print("ğŸš€ Starting chatbot application...")
        print("ğŸ“± Chatbot will be available at: http://localhost:8501")
        
        # Run chatbot in foreground (this will block)
        subprocess.run(chatbot_cmd)
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Chatbot stopped by user")
    except Exception as e:
        print(f"âŒ Error launching chatbot: {e}")

def show_system_status():
    """Show the current system status and access points."""
    print("\n" + "=" * 80)
    print("ğŸ‰ LOCAL MLOPS PIPELINE COMPLETE!")
    print("=" * 80)
    print("ğŸ“Š System Status:")
    print("   âœ… MLflow Server: http://127.0.0.1:5000 (in separate terminal)")
    print("   âœ… Working Monitoring Dashboard: http://localhost:8502 (in separate terminal)")
    print("   âœ… Local Model Storage: ./models/")
    print("   âœ… Chatbot Application: http://localhost:8501")
    print("=" * 80)
    print("ğŸ–¥ï¸  Terminal Windows:")
    print("   â€¢ MLflow Server - Running in 'MLflow Server' terminal")
    print("   â€¢ Working Monitoring Dashboard - Running in 'Working Monitoring Dashboard' terminal")
    print("   â€¢ Main Pipeline - Running in current terminal")
    print("=" * 80)
    print("ğŸ¯ What's Available:")
    print("   â€¢ Model training with MLOps tracking")
    print("   â€¢ Local model deployment (no Docker)")
    print("   â€¢ Interactive travel advisor chatbot")
    print("   â€¢ Model versioning and experiment tracking")
    print("   â€¢ Enhanced dataset with 9,084 destinations")
    print("   â€¢ Working monitoring dashboard with real model data")
    print("   â€¢ Simulated performance metrics for demonstration")
    print("=" * 80)
    print("ğŸ’¡ Local MLOps Benefits:")
    print("   â€¢ No Docker complexity - runs directly on your machine")
    print("   â€¢ Fast startup and deployment")
    print("   â€¢ Easy debugging and development")
    print("   â€¢ All data tracked in MLflow UI")
    print("   â€¢ Separate terminal windows for easy monitoring")
    print("   â€¢ Auto-opened browser links for quick access")
    print("=" * 80)
    print("ğŸ”§ Recent Updates:")
    print("   â€¢ Fixed monitoring dashboard Streamlit warnings")
    print("   â€¢ Created working monitoring dashboard with real model detection")
    print("   â€¢ Bypassed MLflow database corruption issues")
    print("   â€¢ Added simulated performance metrics for demonstration")
    print("=" * 80)

def main():
    """Main MLOps pipeline function."""
    print_banner()
    
    # Step 1: Check dependencies
    if not check_dependencies():
        print("\nâŒ Please install missing dependencies and try again.")
        return
    
    # Step 2: Start MLflow server in new Windows terminal
    print("\nğŸš€ Starting MLflow server in new Windows terminal...")
    if not start_mlflow_server():
        print("\nâŒ Failed to start MLflow server automatically.")
        print("ğŸ’¡ Please start MLflow server manually:")
        print("   mlflow ui --backend-store-uri file:///D:/Travel%20Agent%20Chat%20Bot/mlruns --default-artifact-root file:///D:/Travel%20Agent%20Chat%20Bot/mlruns --host 127.0.0.1 --port 5000")
        print("ğŸ”„ Or continue without MLflow tracking...")
        user_input = input("\nContinue without MLflow? (y/n): ").lower()
        if user_input != 'y':
            return
    
    # Step 3: Train models
    if not train_models():
        print("\nâŒ Model training failed. Exiting.")
        return
    
    # Step 4: Deploy models locally
    if not deploy_models():
        print("\nâŒ Local model deployment failed. Exiting.")
        return
    
    # Step 5: Setup monitoring
    start_monitoring()
    
    # Step 6: Open browser links
    open_browser_links()
    
    # Step 7: Show system status
    show_system_status()
    
    # Step 8: Launch chatbot (this will run in foreground)
    try:
        launch_chatbot()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Local MLOps pipeline stopped by user")
        print("ğŸ”„ To restart, run: python mlops_pipeline.py")

if __name__ == "__main__":
    main()
