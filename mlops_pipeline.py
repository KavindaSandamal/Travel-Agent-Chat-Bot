#!/usr/bin/env python3
"""
Complete MLOps Pipeline for Travel Advisor Chatbot
Single script that handles training, deployment, monitoring, and chatbot launch
"""

import sys
import os
import time
import subprocess
import threading
from datetime import datetime
from typing import Dict, List, Any

# Add project paths
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'src'))
sys.path.append(os.path.join(project_root, 'mlops'))

def print_banner():
    """Print the MLOps pipeline banner."""
    print("=" * 80)
    print("ğŸš€ TRAVEL ADVISOR CHATBOT - COMPLETE MLOPS PIPELINE")
    print("=" * 80)
    print("ğŸ¯ Single Script for Training, Deployment, Monitoring & Chatbot")
    print("=" * 80)

def check_dependencies():
    """Check if all required dependencies are installed."""
    print("\nğŸ” Checking Dependencies...")
    
    # Map package names to import names
    required_packages = {
        'mlflow': 'mlflow',
        'streamlit': 'streamlit', 
        'pandas': 'pandas',
        'numpy': 'numpy',
        'scikit-learn': 'sklearn',
        'sentence-transformers': 'sentence_transformers',
        'transformers': 'transformers',
        'torch': 'torch',
        'nltk': 'nltk'
    }
    
    missing_packages = []
    for package_name, import_name in required_packages.items():
        try:
            __import__(import_name)
            print(f"âœ… {package_name}")
        except ImportError:
            print(f"âŒ {package_name} - Missing")
            missing_packages.append(package_name)
    
    if missing_packages:
        print(f"\nâš ï¸  Missing packages: {', '.join(missing_packages)}")
        print("Please install them with: pip install " + " ".join(missing_packages))
        return False
    
    print("âœ… All dependencies are installed!")
    return True

def check_mlflow_server():
    """Check if MLflow server is running."""
    print("\nğŸ“Š Checking MLflow Server...")
    
    try:
        import requests
        response = requests.get("http://127.0.0.1:5000", timeout=2)
        if response.status_code == 200:
            print("âœ… MLflow server is running on http://127.0.0.1:5000")
            return True
        else:
            print("âŒ MLflow server not responding")
            return False
    except:
        print("âŒ MLflow server not running")
        print("ğŸ’¡ To start MLflow server, run:")
        print("   mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 127.0.0.1 --port 5000")
        return False

def train_models():
    """Train all chatbot models with MLOps tracking."""
    print("\nğŸ“ Training Chatbot Models...")
    print("-" * 50)
    
    try:
        # Import and run training
        from mlops.scripts.train_chatbot_models import ChatbotModelTrainer
        
        trainer = ChatbotModelTrainer()
        
        # Load data and initialize models
        if not trainer.load_training_data():
            print("âŒ Failed to load training data")
            return False
        
        trainer.initialize_models()
        
        # Train all models
        success = trainer.train_all_models()
        
        if success:
            print("\nâœ… All models trained successfully!")
            trainer.evaluate_models()
            trainer.save_training_results()
            return True
        else:
            print("âŒ Model training failed")
            return False
            
    except Exception as e:
        print(f"âŒ Error during training: {e}")
        return False

def deploy_models():
    """Deploy trained models locally (no Docker)."""
    print("\nğŸš€ Deploying Models Locally...")
    print("-" * 35)
    
    try:
        # Create models directory if it doesn't exist
        os.makedirs("models", exist_ok=True)
        
        # Check if models were trained
        models_dir = "models"
        if os.path.exists(models_dir):
            model_files = [f for f in os.listdir(models_dir) if f.endswith(('.pkl', '.joblib', '.pt', '.pth'))]
            if model_files:
                print(f"âœ… Found {len(model_files)} trained models")
                print("âœ… Models are ready for local deployment")
                return True
            else:
                print("âš ï¸  No trained models found, but models directory exists")
                print("âœ… Local deployment setup complete")
                return True
        else:
            print("âœ… Created models directory for local deployment")
            return True
            
    except Exception as e:
        print(f"âŒ Error during local deployment: {e}")
        return False

def start_monitoring():
    """Start monitoring dashboard locally."""
    print("\nğŸ“ˆ Starting Local Monitoring Dashboard...")
    
    try:
        # Check if monitoring dashboard can be imported
        from mlops.dashboards.mlops_monitoring_dashboard import MLOpsMonitoringDashboard
        
        print("âœ… Monitoring dashboard components loaded")
        print("ğŸ’¡ To start monitoring dashboard, run:")
        print("   python mlops/dashboards/mlops_monitoring_dashboard.py")
        print("   Then access: http://localhost:8502")
        return True
        
    except Exception as e:
        print(f"âš ï¸  Monitoring dashboard not available: {e}")
        print("ğŸ’¡ You can still use MLflow UI for monitoring: http://127.0.0.1:5000")
        return True

def launch_chatbot():
    """Launch the Streamlit chatbot application."""
    print("\nğŸ¤– Launching Travel Advisor Chatbot...")
    print("-" * 40)
    
    try:
        # Start chatbot app
        chatbot_cmd = ["streamlit", "run", "travel_chatbot_app.py", "--server.port", "8501"]
        
        print("ğŸš€ Starting chatbot application...")
        print("ğŸ“± Chatbot will be available at: http://localhost:8501")
        
        # Run chatbot in foreground (this will block)
        subprocess.run(chatbot_cmd)
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Chatbot stopped by user")
    except Exception as e:
        print(f"âŒ Error launching chatbot: {e}")

def show_system_status():
    """Show the current system status and access points."""
    print("\n" + "=" * 80)
    print("ğŸ‰ LOCAL MLOPS PIPELINE COMPLETE!")
    print("=" * 80)
    print("ğŸ“Š System Status:")
    print("   âœ… MLflow Server: http://127.0.0.1:5000")
    print("   âœ… Local Model Storage: ./models/")
    print("   âœ… Chatbot Application: http://localhost:8501")
    print("=" * 80)
    print("ğŸ¯ What's Available:")
    print("   â€¢ Model training with MLOps tracking")
    print("   â€¢ Local model deployment (no Docker)")
    print("   â€¢ Interactive travel advisor chatbot")
    print("   â€¢ Model versioning and experiment tracking")
    print("   â€¢ Enhanced dataset with 9,084 destinations")
    print("=" * 80)
    print("ğŸ’¡ Local MLOps Benefits:")
    print("   â€¢ No Docker complexity - runs directly on your machine")
    print("   â€¢ Fast startup and deployment")
    print("   â€¢ Easy debugging and development")
    print("   â€¢ All data tracked in MLflow UI")
    print("=" * 80)

def main():
    """Main MLOps pipeline function."""
    print_banner()
    
    # Step 1: Check dependencies
    if not check_dependencies():
        print("\nâŒ Please install missing dependencies and try again.")
        return
    
    # Step 2: Check MLflow server
    if not check_mlflow_server():
        print("\nâš ï¸  MLflow server not running. Please start it manually.")
        print("ğŸ’¡ Run: mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 127.0.0.1 --port 5000")
        print("ğŸ”„ Or continue without MLflow tracking...")
        user_input = input("\nContinue without MLflow? (y/n): ").lower()
        if user_input != 'y':
            return
    
    # Step 3: Train models
    if not train_models():
        print("\nâŒ Model training failed. Exiting.")
        return
    
    # Step 4: Deploy models locally
    if not deploy_models():
        print("\nâŒ Local model deployment failed. Exiting.")
        return
    
    # Step 5: Setup monitoring
    start_monitoring()
    
    # Step 6: Show system status
    show_system_status()
    
    # Step 7: Launch chatbot (this will run in foreground)
    try:
        launch_chatbot()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Local MLOps pipeline stopped by user")
        print("ğŸ”„ To restart, run: python mlops_pipeline.py")

if __name__ == "__main__":
    main()
